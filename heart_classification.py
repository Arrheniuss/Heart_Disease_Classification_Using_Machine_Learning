# -*- coding: utf-8 -*-
"""Heart_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dMdBr5WiD8qJ56QyCOvKBxDHGaE8Tm-B
"""

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, roc_auc_score,confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

#pip install catboost
from catboost import CatBoostClassifier

def remove_outliers_column(df, column):
    # Obliczanie kwartyliw i IQR dla określonej kolumny
    Q1 = df[column].quantile(0.25)  # Pierwszy kwartyl (Q1)
    Q3 = df[column].quantile(0.75)  # Trzeci kwartyl (Q3)
    IQR = Q3 - Q1  # Rozstęp międzykwartylowy (IQR)

    # Definiowanie dolnej i górnej granicy
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Użycie Series.clip do ograniczenia wartości do zakresu
    df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)

    return df

# Funkcja zbierajaca wyniki modelu
def update_results(model_name, model, X_train, y_train, X_test, y_test):
    # Trenowanie modelu
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    # Obliczanie metryk
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    # AUC tylko, jeśli model ma predict_proba
    if y_pred_proba is not None:
        roc_auc = roc_auc_score(y_test, y_pred_proba)
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
        roc_curves[model_name] = (fpr, tpr)
    else:
        roc_auc = None

    # Dodanie wyników do słownika
    metrics_results['model'].append(model_name)
    metrics_results['accuracy'].append(accuracy)
    metrics_results['precision'].append(precision)
    metrics_results['recall'].append(recall)
    metrics_results['f1'].append(f1)
    metrics_results['roc_auc'].append(roc_auc)

# Inicjalizacja słownika do przechowywania wyników
metrics_results = {
    'model': [],
    'accuracy': [],
    'precision': [],
    'recall': [],
    'f1': [],
    'roc_auc': []
}

roc_curves = {}

df = pd.read_csv('heart.csv')
df_original = df.copy()

print(df.info())

df.describe()

print(df.isnull().sum())

print((df == 0).sum())

print('Wykresy z wartosciamy odstajacymi przed scieciem wartosci odstajacych')
columns = ['RestingBP', 'Cholesterol','MaxHR','Oldpeak']
sns.set_style("darkgrid")
plt.figure(figsize=(12, 8))
for i, column in enumerate(columns):
    plt.subplot(3, 3, i + 1)
    sns.boxplot(y=df[column])
    plt.title(column)
plt.tight_layout()
plt.show()

remove_outliers_column(df, 'RestingBP')
remove_outliers_column(df, 'Cholesterol')
remove_outliers_column(df, 'MaxHR')
remove_outliers_column(df, 'Oldpeak')

print('Wykresy z wartosciamy odstajacymi po scieciu wartosci odstajacych')
columns = ['RestingBP', 'Cholesterol','MaxHR','Oldpeak']
sns.set_style("darkgrid")
plt.figure(figsize=(12, 8))
for i, column in enumerate(columns):
    plt.subplot(3, 3, i + 1)
    sns.boxplot(y=df[column])
    plt.title(column)
plt.tight_layout()
plt.show()

#rozbicie kolumn za pomocą get_dummies
df = pd.get_dummies(df, columns=['Sex'], drop_first=True)
df = pd.get_dummies(df, columns=['ChestPainType'], drop_first=False)
df = pd.get_dummies(df, columns=['RestingECG'], drop_first=False)
df = pd.get_dummies(df, columns=['ExerciseAngina'], drop_first=True)
df = pd.get_dummies(df, columns=['ST_Slope'], drop_first=False)
# Sprawdzenie, jak wyglądają kolumny po przekształceniu
print(df.columns)

# Skalowanie zmiennych ciaglych
columns_to_scale = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR','Oldpeak']
scaler = MinMaxScaler()
df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])

# Skalowanie zmiennych kategorycznych
label_encoder = LabelEncoder()

df['Sex_M'] = label_encoder.fit_transform(df['Sex_M'])
df['ChestPainType_ASY'] = label_encoder.fit_transform(df['ChestPainType_ASY'])
df['ChestPainType_ATA'] = label_encoder.fit_transform(df['ChestPainType_ATA'])
df['ChestPainType_NAP'] = label_encoder.fit_transform(df['ChestPainType_NAP'])
df['ChestPainType_TA'] = label_encoder.fit_transform(df['ChestPainType_TA'])
df['RestingECG_LVH'] = label_encoder.fit_transform(df['RestingECG_LVH'])
df['RestingECG_Normal'] = label_encoder.fit_transform(df['RestingECG_Normal'])
df['RestingECG_ST'] = label_encoder.fit_transform(df['RestingECG_ST'])
df['ExerciseAngina_Y'] = label_encoder.fit_transform(df['ExerciseAngina_Y'])
df['ST_Slope_Down'] = label_encoder.fit_transform(df['ST_Slope_Down'])
df['ST_Slope_Flat'] = label_encoder.fit_transform(df['ST_Slope_Flat'])
df['ST_Slope_Up'] = label_encoder.fit_transform(df['ST_Slope_Up'])

# Macierz korelacji
corr_matrix = df.drop(['ChestPainType_ATA',
       'ChestPainType_NAP', 'ChestPainType_TA','ChestPainType_ASY', 'RestingECG_LVH',
       'RestingECG_Normal', 'RestingECG_ST','ExerciseAngina_Y', 'ST_Slope_Down', 'ST_Slope_Flat', 'ST_Slope_Up'], axis=1).corr()

sns.heatmap(corr_matrix, annot=True, cmap='Reds')
plt.show()

corr_matrix = df.drop(['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak', 'Sex_M'], axis=1).corr()
plt.figure(figsize=(12, 6))
sns.heatmap(corr_matrix, annot=True, cmap='Reds')
plt.show()

X = df.drop('HeartDisease', axis=1)
# Zmienna docelowa (y) - kolumna target
y = df['HeartDisease']
# Podział danych - 80% na trening, 20% na testy
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Wypisujemy rozmiary danych
print(f"Rozmiar zbioru treningowego: {X_train.shape}")
print(f"Rozmiar zbioru testowego: {X_test.shape}")

# Tworzymy model KNN z 5 sąsiadami
knn_model = KNeighborsClassifier(n_neighbors=5)

# Dodanie wynikow do sekcji
update_results('KNN', knn_model, X_train, y_train, X_test, y_test)

# Trenowanie modelu na zestawie treningowym
knn_model.fit(X_train, y_train)

# Przewidywanie wyników na zbiorze testowym
y_pred = knn_model.predict(X_test)

# Obliczenie macierzy pomyłek
cm = confusion_matrix(y_test, y_pred)
print("Macierz pomyłek:")
print(cm)


# Ocena modelu za pomocą czułości (recall)

accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Wyświetlenie wyników
print('Model KNN')
print(f"Czułość: {recall:.2f}")
print(f"Dokładnosc: {accuracy:.2f}")
print(f"Precyzja: {precision:.2f}")
print(f"Wskaźnik F1: {f1:.2f}")

# GaussianNB
gnb = GaussianNB()

# Trenowanie modelu na zbiorze treningowym
gnb.fit(X_train, y_train)

# Predykcja na zbiorze testowym
y_pred = gnb.predict(X_test)

# Dodanie wynikow do sekcji
update_results('GausianNB', gnb, X_train, y_train, X_test, y_test)

# Ewaluacja modelu
recall = recall_score(y_test, y_pred)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Obliczenie macierzy pomyłek
cm = confusion_matrix(y_test, y_pred)
print("Macierz pomyłek:")
print(cm)

# Wyświetlenie wyników
print('Model GaussianNB')
print(f"Czułość: {recall:.2f}")
print(f"Dokładność: {accuracy:.2f}")
print(f"Precyzja: {precision:.2f}")
print(f"Wskaźnik F1: {f1:.2f}")

# Tworzenie modelu drzewa decyzyjnego
dtc = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)

# Dodanie wynikow do sekcji
update_results('DecisionTreeClassifier', dtc, X_train, y_train, X_test, y_test)

# Trenowanie modelu
dtc.fit(X_train, y_train)

# Predykcje na zbiorze testowym
y_pred = dtc.predict(X_test)

# Obliczenie metryk
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Obliczenie macierzy pomyłek
cm = confusion_matrix(y_test, y_pred)
print("Macierz pomyłek:")
print(cm)

print('Model drzew decyzyjnych')
print(f"Dokładność: {accuracy:.2f}")
print(f"Czułość: {recall:.2f}")
print(f"Precyzja: {precision:.2f}")
print(f"Wskaźnik F1: {f1:.2f}")

# Tworzenie modelu drzew losowych
rf_clf = RandomForestClassifier(n_estimators=100,        # Liczba drzew w lesie
                                criterion='gini',        # Użycie Gini do podziału węzłów
                                max_depth=15,            # Maksymalna głębokość drzew
                                min_samples_split=5,     # Minimalna liczba próbek do podziału węzła
                                min_samples_leaf=2,      # Minimalna liczba próbek w liściu
                                max_features='sqrt',     # Liczba cech do rozważenia przy każdym podziale
                                random_state=42)
# Dodanie wynikow do sekcji
update_results('RandomForestClassifier', rf_clf, X_train, y_train, X_test, y_test)

# Trenowanie modelu
rf_clf.fit(X_train, y_train)

# Predykcje
y_pred = rf_clf.predict(X_test)

# Obliczanie metryk
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Obliczenie macierzy pomyłek
cm = confusion_matrix(y_test, y_pred)
print("Macierz pomyłek:")
print(cm)

print('Model lasu drzew losowych')
print(f"Dokładność: {accuracy:.2f}")
print(f"Czułość: {recall:.2f}")
print(f"Precyzja: {precision:.2f}")
print(f"Wskaźnik F1: {f1:.2f}")

# Tworzenie modelu regresji logistycznej z niestandardowymi hiperparametrami
log_reg = LogisticRegression(solver='liblinear', penalty='l2', C=1, max_iter=1000, random_state=42)

#Dodanie wynikow do sekcji
update_results('LogisticRegression', log_reg, X_train, y_train, X_test, y_test)

# Trenowanie modelu
log_reg.fit(X_train, y_train)

# Predykcja na zbiorze testowym
y_pred = log_reg.predict(X_test)

# Obliczanie metryk
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Obliczenie macierzy pomyłek
cm = confusion_matrix(y_test, y_pred)
print("Macierz pomyłek:")
print(cm)

print('Model regresji logistycznej')
print(f"Dokładność: {accuracy:.2f}")
print(f"Czułość: {recall:.2f}")
print(f"Precyzja: {precision:.2f}")
print(f"Wskaźnik F1: {f1:.2f}")
print('\n')

# Tworzenie modelu maszyny wektorów nośnych z wyznaczaniem prawdopodobieństw
svc_model = SVC(kernel='poly', C=2.0, probability=True, random_state=42)

#Dodanie wynikow do sekcji
update_results('SVC', svc_model, X_train, y_train, X_test, y_test)


# Trenowanie modelu
svc_model.fit(X_train, y_train)

# Predykcja na zbiorze testowym
y_pred = svc_model.predict(X_test)

# Obliczanie metryk
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Obliczenie macierzy pomyłek
cm = confusion_matrix(y_test, y_pred)
print("Macierz pomyłek:")
print(cm)

print('Model SVC')
print(f"Dokładność: {accuracy:.2f}")
print(f"Czułość: {recall:.2f}")
print(f"Precyzja: {precision:.2f}")
print(f"Wskaźnik F1: {f1:.2f}")

# XGBOOST

# Tworzenie modelu
ada_model = AdaBoostClassifier(n_estimators=50)
ada_model.fit(X_train, y_train)

#Dodanie wynikow do sekcji
update_results('AdaModel', ada_model, X_train, y_train, X_test, y_test)


y_pred = ada_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Obliczenie macierzy pomyłek
cm = confusion_matrix(y_test, y_pred)
print("Macierz pomyłek:")
print(cm)

print('Model AdaBoost')
print(f'Dokładność: {accuracy:.2f}')
print(f"Czułość: {recall:.2f}")
print(f"Precyzja: {precision:.2f}")
print(f"Wskaźnik F1: {f1:.2f}")

# Extra Trees Classifier
et_model = ExtraTreesClassifier(n_estimators=250, random_state=42)
et_model.fit(X_train, y_train)

# Dodanie wynikow do sekcji
update_results('Et_model', et_model, X_train, y_train, X_test, y_test)

# Predykcja na zbiorze testowym
y_pred = et_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Obliczenie macierzy pomyłek
cm = confusion_matrix(y_test, y_pred)
print("Macierz pomyłek:")
print(cm)

print('Model Extra Trees Classifier')
print(f'Dokładność: {accuracy:.2f}')
print(f"Czułość: {recall:.2f}")
print(f"Precyzja: {precision:.2f}")
print(f"Wskaźnik F1: {f1:.2f}")

# QDA

qda_model = QuadraticDiscriminantAnalysis()
qda_model.fit(X_train, y_train)

# Dodanie wynikow do sekcji
update_results('qda_model', qda_model, X_train, y_train, X_test, y_test)

# Predykcja na zbiorze testowym
y_pred = qda_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Obliczenie macierzy pomyłek
cm = confusion_matrix(y_test, y_pred)
print("Macierz pomyłek:")
print(cm)

print('Model QDA')
print(f'Dokładność: {accuracy:.2f}')
print(f"Czułość: {recall:.2f}")
print(f"Precyzja: {precision:.2f}")
print(f"Wskaźnik F1: {f1:.2f}")
print('\n')

# CatBoost
cat_model = CatBoostClassifier(
    iterations=100,          # liczba iteracji
    depth=5,                 # głębokość drzewa
    learning_rate=0.1,       # współczynnik uczenia się
    verbose=10               # częstotliwość wyświetlania informacji o procesie treningu
)

update_results('CatBoost', cat_model, X_train, y_train, X_test, y_test)

# Trenowanie modelu
cat_model.fit(X_train, y_train)

# Predykcja i ewaluacja
y_pred = cat_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Obliczenie macierzy pomyłek
cm = confusion_matrix(y_test, y_pred)
print("Macierz pomyłek:")
print(cm)

print('Model CatBoost')
print(f'Dokładność: {accuracy:.2f}')
print(f"Czułość: {recall:.2f}")
print(f"Precyzja: {precision:.2f}")
print(f"Wskaźnik F1: {f1:.2f}")

#krzywa ROC
plt.figure(figsize=(12, 9))

for model_name, (fpr, tpr) in roc_curves.items():
    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc(fpr, tpr):.2f})')

# Liniowa krzywa dla losowego klasyfikatora
plt.plot([0, 1], [0, 1], 'k--', label='Random')

# Tytuł i etykiety
plt.title('Krzywa ROC dla różnych modeli')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()

plt.show()

results_df = pd.DataFrame(metrics_results)
# Ranking na podstawie czułości
ranked_models_recall = results_df.sort_values(by='recall', ascending=False).reset_index(drop=True)
ranked_models_recall['Rank'] = ranked_models_recall.index + 1
print("\nRanking modeli na podstawie czułości (recall):")
print(ranked_models_recall[['Rank', 'model', 'recall']].to_string(index=False))

# Ranking na podstawie F1-score
ranked_models_f1 = results_df.sort_values(by='f1', ascending=False).reset_index(drop=True)
ranked_models_f1['Rank'] = ranked_models_f1.index + 1
print("\nRanking modeli na podstawie F1-score:")
print(ranked_models_f1[['Rank', 'model', 'f1']].to_string(index=False))

# Ranking na podstawie AUC
ranked_models_auc = results_df.sort_values(by='roc_auc', ascending=False).reset_index(drop=True)
ranked_models_auc['Rank'] = ranked_models_auc.index + 1
print("\nRanking modeli na podstawie AUC:")
print(ranked_models_auc[['Rank', 'model', 'roc_auc']].to_string(index=False))

"""**1. Ogólna skuteczność modeli w różnych metrykach**

*   CatBoost jest jednym z najlepszych modeli, zajmując wysokie pozycje w rankingu dla wszystkich metryk. Wyróżnia się jako pierwszy w F1-score i ma drugą pozycję w rankingu AUC oraz czułości. Jest to więc model, który dobrze balansuje między precyzją a czułością

*   RandomForestClassifier zajmuje pierwszą pozycję w rankingu AUC i wysoką pozycję w F1-score i czułości. Jest to wszechstronny model o dobrych wynikach, zwłaszcza w ocenie skuteczności klasyfikacji (AUC) i równoważeniu czułości oraz precyzji (F1)

*   Et_model (Extra Trees) regularnie osiąga wysokie wyniki w czułości i F1-score oraz zajmuje trzecie miejsce w AUC. Podobnie jak CatBoost i RandomForest, Extra Trees sprawdza się dobrze w równoważeniu między czułością a precyzją.

**2. Szczególne przypadki modeli i ich zastosowania**

*   qda_model osiąga najlepszą pozycję w czułości, ale wypada słabiej pod względem F1-score (dopiero 13. miejsce) i AUC (ostatnie miejsce). Wysoka czułość oznacza, że model jest dobry w minimalizowaniu fałszywych negatywów, co może być przydatne, jeśli kluczowe jest wykrycie wszystkich pozytywnych przypadków jak np test na covid

*   LogisticRegression utrzymuje dość stabilną pozycję w trzech rankingach, zajmując miejsca między 5. a 8. Jest więc wszechstronny, ale niekoniecznie najlepszy w żadnym aspekcie


*   K-Nearest Neighbors (KNN) jest jednym z lepiej ocenianych modeli w czułości i F1-score, ale jego wynik AUC plasuje go na niższej pozycji. KNN może być przydatny w kontekście, gdzie kluczowe jest zbalansowanie trafności wyników

**3. Ogólna stabilność wyników w zależności od metryki**



*   Modele takie jak CatBoost, RandomForestClassifier i Et_model wykazują stabilność między rankingami, co sugeruje ich ogólną przydatność w różnych warunkach

*   Modele jak qda_model i AdaModel mają bardziej specyficzne zastosowania – qda_model jest lepszy w wykrywaniu pozytywnych przypadków (czułość), ale AdaModel jest stabilniejszy pod kątem ogólnej skuteczności (AUC)

4. Dobór metryki do oceny modelu

*   Czułość (recall): Wysoka czułość jest kluczowa, jeśli projekt wymaga minimalizacji pominiętych przypadków pozytywnych (np. w wykrywaniu chorób, oszustw). W takim przypadku można rozważyć qda_model lub CatBoost

*   F1-score: F1-score jest przydatny, gdy klasy są niezrównoważone, a zależy nam na równoważeniu między precyzją i czułością. Tu CatBoost, Et_model i RandomForest mają najlepsze wyniki

*   AUC: Pomaga ocenić, jak dobrze model różnicuje klasy ogólnie, niezależnie od progu decyzyjnego. RandomForestClassifier i CatBoost mają tutaj najwyższe wyniki, więc są najlepsze pod względem ogólnej trafności




















"""